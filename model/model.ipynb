{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiiT7kYEUSFJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Конфигурация\n",
        "CONFIG = {\n",
        "    'tfidf': {\n",
        "        'max_features': 10000,\n",
        "        'ngram_range': (1, 2)\n",
        "    },\n",
        "    'svd': {\n",
        "        'n_components': 300\n",
        "    },\n",
        "    'model': {\n",
        "        'hidden_dim': 256,\n",
        "        'dropout': 0.5,\n",
        "        'bert_train_layers': 4,\n",
        "        'lr': {\n",
        "            'model': 1e-4,\n",
        "            'bert': 2e-5\n",
        "        }\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 32,\n",
        "        'epochs': 10,\n",
        "        'warmup_steps': 100,\n",
        "        'val_size': 0.2,\n",
        "        'random_state': 42,\n",
        "        'n_folds': 5,\n",
        "        'patience': 3       # Новый параметр\n",
        "    }\n",
        "}\n",
        "\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, tfidf_proj, bert_proj):\n",
        "        q = self.query(bert_proj).unsqueeze(2)\n",
        "        k = self.key(tfidf_proj).unsqueeze(1)\n",
        "        v = self.value(tfidf_proj).unsqueeze(1)\n",
        "        scores = torch.bmm(k, q) / (self.key.out_features ** 0.5)\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        return torch.bmm(attention.transpose(1, 2), v).squeeze(1)\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, tfidf_dim, bert_dim, hidden_dim=256, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.tfidf_proj = nn.Sequential(\n",
        "            nn.Linear(tfidf_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "        self.bert_proj = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "        self.cross_attn = CrossModalAttention(hidden_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, tfidf_features, bert_features):\n",
        "        tfidf_proj = self.tfidf_proj(tfidf_features)\n",
        "        bert_proj = self.bert_proj(bert_features)\n",
        "        attended = self.cross_attn(tfidf_proj, bert_proj)\n",
        "        return self.classifier(attended)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tfidf_vectorizer, svd, tokenizer, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tfidf_vectorizer = tfidf_vectorizer\n",
        "        self.svd = svd\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tfidf_features = self.tfidf_vectorizer.transform([text]).toarray()[0]\n",
        "        tfidf_features = self.svd.transform(tfidf_features.reshape(1, -1))[0]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'tfidf_features': torch.FloatTensor(tfidf_features),\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "        return item\n",
        "\n",
        "def evaluate(model, val_loader, bert_model, device):\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            bert_outputs = bert_model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            outputs = model(\n",
        "                batch['tfidf_features'],\n",
        "                bert_outputs.last_hidden_state[:, 0, :]\n",
        "            )\n",
        "            preds.extend(outputs.cpu().numpy().flatten())\n",
        "            labels.extend(batch['labels'].cpu().numpy().flatten())\n",
        "\n",
        "    return {\n",
        "        'auc': roc_auc_score(labels, preds),\n",
        "        'f1': f1_score(labels, (np.array(preds) > 0.5).astype(int)),\n",
        "        'acc': accuracy_score(labels, (np.array(preds) > 0.5).astype(int))\n",
        "    }\n",
        "\n",
        "def train_fold(train_df, val_df, test_texts, fold, device):\n",
        "    tfidf_vectorizer = TfidfVectorizer(**CONFIG['tfidf'])\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['text'])\n",
        "    svd = TruncatedSVD(**CONFIG['svd']).fit(tfidf_matrix)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        TextDataset(train_df['text'].tolist(), tfidf_vectorizer, svd, tokenizer, train_df['label'].tolist()),\n",
        "        batch_size=CONFIG['training']['batch_size'],\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        TextDataset(val_df['text'].tolist(), tfidf_vectorizer, svd, tokenizer, val_df['label'].tolist()),\n",
        "        batch_size=CONFIG['training']['batch_size']\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        TextDataset(test_texts, tfidf_vectorizer, svd, tokenizer),\n",
        "        batch_size=CONFIG['training']['batch_size']\n",
        "    )\n",
        "\n",
        "    bert_model = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "    for param in bert_model.parameters():\n",
        "        param.requires_grad = False\n",
        "    for layer in bert_model.encoder.layer[-CONFIG['model']['bert_train_layers']:]:\n",
        "        for param in layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    model = HybridModel(\n",
        "        tfidf_dim=CONFIG['svd']['n_components'],\n",
        "        bert_dim=768,\n",
        "        hidden_dim=CONFIG['model']['hidden_dim'],\n",
        "        dropout=CONFIG['model']['dropout']\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': model.parameters(), 'lr': CONFIG['model']['lr']['model']},\n",
        "        {'params': bert_model.parameters(), 'lr': CONFIG['model']['lr']['bert']}\n",
        "    ])\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=CONFIG['training']['warmup_steps'],\n",
        "        num_training_steps=len(train_loader) * CONFIG['training']['epochs']\n",
        "    )\n",
        "\n",
        "    best_auc = 0\n",
        "    for epoch in range(CONFIG['training']['epochs']):\n",
        "        model.train()\n",
        "        for batch in tqdm(train_loader, desc=f'Fold {fold} Epoch {epoch}'):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            bert_outputs = bert_model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            outputs = model(\n",
        "                batch['tfidf_features'],\n",
        "                bert_outputs.last_hidden_state[:, 0, :]\n",
        "            )\n",
        "            loss = F.binary_cross_entropy(outputs, batch['labels'])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        val_metrics = evaluate(model, val_loader, bert_model, device)\n",
        "        if val_metrics['auc'] > best_auc:\n",
        "            best_auc = val_metrics['auc']\n",
        "            torch.save(model.state_dict(), f'best_fold{fold}.pt')\n",
        "\n",
        "    model.load_state_dict(torch.load(f'best_fold{fold}.pt'))\n",
        "    test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            bert_outputs = bert_model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            outputs = model(\n",
        "                batch['tfidf_features'],\n",
        "                bert_outputs.last_hidden_state[:, 0, :]\n",
        "            )\n",
        "            test_preds.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    return best_auc, np.array(test_preds)\n",
        "\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train = pd.read_csv(\"train.csv\")\n",
        "    test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=CONFIG['training']['n_folds'], shuffle=True,\n",
        "                         random_state=CONFIG['training']['random_state'])\n",
        "\n",
        "    fold_aucs = []\n",
        "    test_preds = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['label'])):\n",
        "        train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "        auc, preds = train_fold(train_df, val_df, test['text'].tolist(), fold, device)\n",
        "        fold_aucs.append(auc)\n",
        "        test_preds += preds / CONFIG['training']['n_folds']\n",
        "\n",
        "    print(f\"Mean AUC: {np.mean(fold_aucs):.4f}\")\n",
        "    pd.DataFrame({'id': test['id'], 'generated': test_preds}).to_csv('submission.csv', index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}