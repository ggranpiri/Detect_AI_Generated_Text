{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiiT7kYEUSFJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "\n",
        "# Отключаем предупреждения\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== КОНФИГУРАЦИЯ ====================\n",
        "CONFIG = {\n",
        "    'data': {\n",
        "        'train_path': \"/kaggle/input/train-dataset/train_v2_drcat_02.csv\",\n",
        "        'test_path': \"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\",\n",
        "        'bert_path': \"/kaggle/input/bert-base-uncased\",\n",
        "        'max_length': 128,\n",
        "        'val_size': 0.2,\n",
        "        'random_state': 42,\n",
        "        'n_folds': 5\n",
        "    },\n",
        "    'tfidf': {\n",
        "        'max_features': 10000,\n",
        "        'ngram_range': (1, 2)\n",
        "    },\n",
        "    'svd': {\n",
        "        'n_components': 300\n",
        "    },\n",
        "    'model': {\n",
        "        'hidden_dim': 128,\n",
        "        'dropout': 0.3,\n",
        "        'lr': {\n",
        "            'model': 1e-4,\n",
        "            'bert': 2e-5\n",
        "        }\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 32,\n",
        "        'epochs': 5,\n",
        "        'warmup_steps': 100,\n",
        "        'patience': 2\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==================== 1. ПОДГОТОВКА И АНАЛИЗ ДАННЫХ ====================\n",
        "def clean_text(text):\n",
        "    \"\"\"Очистка текста от артефактов\"\"\"\n",
        "    # Удаление специфичных маркеров\n",
        "    text = re.sub(r'\\[(ai|generated|model).*?\\]', '', text, flags=re.IGNORECASE)\n",
        "    # Удаление повторяющихся пробелов\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def detect_artifacts(texts, labels, n=3):\n",
        "    \"\"\"Поиск характерных n-грамм\"\"\"\n",
        "    ai_ngrams = defaultdict(int)\n",
        "    human_ngrams = defaultdict(int)\n",
        "\n",
        "    for text, label in zip(texts, labels):\n",
        "        words = text.lower().split()\n",
        "        for i in range(len(words)-n+1):\n",
        "            ngram = ' '.join(words[i:i+n])\n",
        "            if label == 1:\n",
        "                ai_ngrams[ngram] += 1\n",
        "            else:\n",
        "                human_ngrams[ngram] += 1\n",
        "\n",
        "    # Находим уникальные для AI n-граммы\n",
        "    unique_ai = {k: v for k, v in ai_ngrams.items() if k not in human_ngrams}\n",
        "    return sorted(unique_ai.items(), key=lambda x: -x[1])[:20]\n",
        "\n",
        "def analyze_dataset(train_df, test_df):\n",
        "    \"\"\"Полный анализ данных\"\"\"\n",
        "    print(\"\\n=== АНАЛИЗ ДАННЫХ ===\")\n",
        "\n",
        "    # 1. Проверка распределения меток\n",
        "    print(\"\\nРаспределение меток в train:\")\n",
        "    print(train_df['label'].value_counts(normalize=True))\n",
        "\n",
        "    # 2. Поиск артефактов\n",
        "    artifacts = detect_artifacts(train_df['text'], train_df['label'])\n",
        "    print(\"\\nТоп-20 уникальных AI-паттернов:\")\n",
        "    for ngram, count in artifacts:\n",
        "        print(f\"{ngram}: {count}\")\n",
        "\n",
        "    # 3. Проверка пересечений train/test\n",
        "    train_texts = set(train_df['text'].str.lower())\n",
        "    test_texts = set(test_df['text'].str.lower())\n",
        "    print(f\"\\nПересекающихся текстов: {len(train_texts & test_texts)}\")\n",
        "\n",
        "    # 4. Визуализация длины текстов\n",
        "    train_df['length'] = train_df['text'].apply(len)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x='label', y='length', data=train_df)\n",
        "    plt.title(\"Распределение длины текстов\")\n",
        "    plt.show()\n",
        "\n",
        "# ==================== 2. ПОДГОТОВКА МОДЕЛИ ====================\n",
        "class HybridModel(nn.Module):\n",
        "    \"\"\"Гибридная модель с кросс-модальным вниманием\"\"\"\n",
        "    def __init__(self, tfidf_dim, bert_dim, hidden_dim=128, dropout=0.3):\n",
        "        super().__init__()\n",
        "        # Проекции для признаков\n",
        "        self.tfidf_proj = nn.Sequential(\n",
        "            nn.Linear(tfidf_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.bert_proj = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "        # Механизм внимания\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim*2, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softmax(dim=1))\n",
        "\n",
        "        # Классификатор\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, tfidf_features, bert_features):\n",
        "        # Проекции признаков\n",
        "        tfidf_proj = self.tfidf_proj(tfidf_features)\n",
        "        bert_proj = self.bert_proj(bert_features)\n",
        "\n",
        "        # Внимание\n",
        "        combined = torch.cat([tfidf_proj.unsqueeze(1), bert_proj.unsqueeze(1)], dim=1)\n",
        "        attn_weights = self.attention(combined)\n",
        "        attended = (combined * attn_weights).sum(dim=1)\n",
        "\n",
        "        return self.classifier(attended)\n",
        "\n",
        "# ==================== 3. ОБУЧЕНИЕ И ВАЛИДАЦИЯ ====================\n",
        "def train_and_validate():\n",
        "    # Инициализация\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Загрузка данных\n",
        "    train = pd.read_csv(CONFIG['data']['train_path'])\n",
        "    test = pd.read_csv(CONFIG['data']['test_path'])\n",
        "\n",
        "    # Очистка данных\n",
        "    train['text'] = train['text'].apply(clean_text)\n",
        "    test['text'] = test['text'].apply(clean_text)\n",
        "\n",
        "    # Анализ данных\n",
        "    analyze_dataset(train, test)\n",
        "\n",
        "    # Подготовка TF-IDF\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=CONFIG['tfidf']['max_features'],\n",
        "        ngram_range=CONFIG['tfidf']['ngram_range'])\n",
        "    svd = TruncatedSVD(n_components=CONFIG['svd']['n_components'])\n",
        "\n",
        "    # Кросс-валидация\n",
        "    skf = StratifiedKFold(\n",
        "        n_splits=CONFIG['data']['n_folds'],\n",
        "        shuffle=True,\n",
        "        random_state=CONFIG['data']['random_state'])\n",
        "\n",
        "    fold_metrics = []\n",
        "    test_preds = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['label'])):\n",
        "        print(f\"\\n=== Fold {fold+1}/{CONFIG['data']['n_folds']} ===\")\n",
        "\n",
        "        # Разделение данных\n",
        "        train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "\n",
        "        # Обучение TF-IDF\n",
        "        tfidf_matrix = tfidf.fit_transform(train_df['text'])\n",
        "        svd.fit(tfidf_matrix)\n",
        "\n",
        "        # Даталоадеры\n",
        "        tokenizer = AutoTokenizer.from_pretrained(CONFIG['data']['bert_path'])\n",
        "\n",
        "        train_dataset = TextDataset(\n",
        "            train_df['text'].tolist(),\n",
        "            tfidf, svd, tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'],\n",
        "            labels=train_df['label'].tolist())\n",
        "\n",
        "        val_dataset = TextDataset(\n",
        "            val_df['text'].tolist(),\n",
        "            tfidf, svd, tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'],\n",
        "            labels=val_df['label'].tolist())\n",
        "\n",
        "        test_dataset = TextDataset(\n",
        "            test['text'].tolist(),\n",
        "            tfidf, svd, tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'],\n",
        "            shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'])\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'])\n",
        "\n",
        "        # Инициализация моделей\n",
        "        bert_model = AutoModel.from_pretrained(CONFIG['data']['bert_path']).to(device)\n",
        "        model = HybridModel(\n",
        "            tfidf_dim=CONFIG['svd']['n_components'],\n",
        "            bert_dim=768,\n",
        "            hidden_dim=CONFIG['model']['hidden_dim'],\n",
        "            dropout=CONFIG['model']['dropout']).to(device)\n",
        "\n",
        "        # Оптимизатор и шедулер\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': model.parameters(), 'lr': CONFIG['model']['lr']['model']},\n",
        "            {'params': bert_model.parameters(), 'lr': CONFIG['model']['lr']['bert']}\n",
        "        ])\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=CONFIG['training']['warmup_steps'],\n",
        "            num_training_steps=len(train_loader)*CONFIG['training']['epochs'])\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Обучение\n",
        "        best_auc = 0\n",
        "        patience = 0\n",
        "\n",
        "        for epoch in range(CONFIG['training']['epochs']):\n",
        "            model.train()\n",
        "            bert_model.train()\n",
        "            total_loss = 0\n",
        "\n",
        "            for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    bert_outputs = bert_model(\n",
        "                        input_ids=batch['input_ids'],\n",
        "                        attention_mask=batch['attention_mask'])\n",
        "\n",
        "                    outputs = model(\n",
        "                        batch['tfidf_features'],\n",
        "                        bert_outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "                    loss = criterion(outputs, batch['labels'])\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()\n",
        "\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "            # Валидация\n",
        "            model.eval()\n",
        "            bert_model.eval()\n",
        "            val_preds, val_labels = [], []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                    bert_outputs = bert_model(\n",
        "                        input_ids=batch['input_ids'],\n",
        "                        attention_mask=batch['attention_mask'])\n",
        "\n",
        "                    outputs = model(\n",
        "                        batch['tfidf_features'],\n",
        "                        bert_outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "                    val_preds.extend(outputs.cpu().numpy().flatten())\n",
        "                    val_labels.extend(batch['labels'].cpu().numpy().flatten())\n",
        "\n",
        "            auc = roc_auc_score(val_labels, val_preds)\n",
        "            print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f}, Val AUC = {auc:.4f}\")\n",
        "\n",
        "            # Ранняя остановка\n",
        "            if auc > best_auc:\n",
        "                best_auc = auc\n",
        "                patience = 0\n",
        "                torch.save(model.state_dict(), f'best_model_fold{fold}.pt')\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience >= CONFIG['training']['patience']:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        # Сохранение метрик\n",
        "        fold_metrics.append(best_auc)\n",
        "\n",
        "        # Предсказание на тесте\n",
        "        model.load_state_dict(torch.load(f'best_model_fold{fold}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        fold_test_preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                bert_outputs = bert_model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'])\n",
        "\n",
        "                outputs = model(\n",
        "                    batch['tfidf_features'],\n",
        "                    bert_outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "                fold_test_preds.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "        test_preds += np.array(fold_test_preds) / CONFIG['data']['n_folds']\n",
        "\n",
        "    # Сохранение результатов\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'generated': test_preds\n",
        "    })\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "    print(\"\\n=== ИТОГОВЫЕ МЕТРИКИ ===\")\n",
        "    print(f\"Средний AUC по фолдам: {np.mean(fold_metrics):.4f} (±{np.std(fold_metrics):.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_validate()"
      ]
    }
  ]
}