{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiiT7kYEUSFJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import re\n",
        "from collections import Counter\n",
        "from captum.attr import IntegratedGradients  # Для интерпретации\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Конфигурация\n",
        "CONFIG = {\n",
        "    'tfidf': {\n",
        "        'max_features': 10000,\n",
        "        'ngram_range': (1, 2)\n",
        "    },\n",
        "    'svd': {\n",
        "        'n_components': 300\n",
        "    },\n",
        "    'model': {\n",
        "        'hidden_dim': 256,  # Увеличено\n",
        "        'dropout': 0.5,\n",
        "        'bert_train_layers': 4,  # Сколько слоёв BERT будем обучать\n",
        "        'lr': {\n",
        "            'model': 1e-4,\n",
        "            'bert': 2e-5\n",
        "        }\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 32,\n",
        "        'epochs': 10,  # Увеличено\n",
        "        'warmup_steps': 100,\n",
        "        'val_size': 0.2,\n",
        "        'random_state': 42,\n",
        "        'n_folds': 5,\n",
        "        'patience': 3  # Для ранней остановки\n",
        "    },\n",
        "    'logging': {\n",
        "        'project': 'ai_text_detection',\n",
        "        'enabled': True\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==================== МОДЕЛЬ ====================\n",
        "class CrossModalAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, tfidf_proj, bert_proj):\n",
        "        q = self.query(bert_proj).unsqueeze(2)\n",
        "        k = self.key(tfidf_proj).unsqueeze(1)\n",
        "        v = self.value(tfidf_proj).unsqueeze(1)\n",
        "        scores = torch.bmm(k, q) / (self.key.out_features ** 0.5)\n",
        "        attention = self.softmax(scores)\n",
        "        return torch.bmm(attention.transpose(1, 2), v).squeeze(1)\n",
        "\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, tfidf_dim, bert_dim, hidden_dim=256, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.tfidf_proj = nn.Sequential(\n",
        "            nn.Linear(tfidf_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "        self.bert_proj = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout))\n",
        "\n",
        "        self.cross_attn = CrossModalAttention(hidden_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, tfidf_features, bert_features):\n",
        "        tfidf_proj = self.tfidf_proj(tfidf_features)\n",
        "        bert_proj = self.bert_proj(bert_features)\n",
        "        attended = self.cross_attn(tfidf_proj, bert_proj)\n",
        "        return self.classifier(attended)\n",
        "\n",
        "# ==================== ДАННЫЕ ====================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tfidf_vectorizer, svd, tokenizer, max_length=128, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tfidf_vectorizer = tfidf_vectorizer\n",
        "        self.svd = svd\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        tfidf_features = self.tfidf_vectorizer.transform([text]).toarray()[0]\n",
        "        tfidf_features = self.svd.transform(tfidf_features.reshape(1, -1))[0]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'tfidf_features': torch.FloatTensor(tfidf_features),\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "        return item\n",
        "\n",
        "# ==================== УТИЛИТЫ ====================\n",
        "def analyze_data(train, test):\n",
        "    \"\"\"Анализ данных без утечек\"\"\"\n",
        "    print(\"\\n=== АНАЛИЗ ДАННЫХ ===\")\n",
        "\n",
        "    # Анализ длины текстов\n",
        "    train['length'] = train['text'].apply(len)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x='label', y='length', data=train)\n",
        "    plt.title(\"Распределение длины текстов по классам\")\n",
        "    plt.show()\n",
        "\n",
        "    # Анализ дисбаланса классов\n",
        "    class_dist = train['label'].value_counts(normalize=True)\n",
        "    print(f\"\\nРаспределение классов:\\n{class_dist}\")\n",
        "\n",
        "    return train\n",
        "\n",
        "def evaluate_model(model, data_loader, bert_model, device):\n",
        "    \"\"\"Расширенная оценка модели\"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_labels, all_probs = [], [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            bert_outputs = bert_model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            outputs = model(\n",
        "                batch['tfidf_features'],\n",
        "                bert_outputs.last_hidden_state[:, 0, :]\n",
        "            )\n",
        "\n",
        "            all_probs.extend(outputs.cpu().numpy().flatten())\n",
        "            preds = (outputs > 0.5).float()\n",
        "            all_preds.extend(preds.cpu().numpy().flatten())\n",
        "            all_labels.extend(batch['labels'].cpu().numpy().flatten())\n",
        "\n",
        "    metrics = {\n",
        "        'auc': roc_auc_score(all_labels, all_probs),\n",
        "        'f1': f1_score(all_labels, all_preds),\n",
        "        'accuracy': accuracy_score(all_labels, all_preds),\n",
        "        'report': classification_report(all_labels, all_preds)\n",
        "    }\n",
        "\n",
        "    # Матрица ошибок\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "def predict_test(model, test_loader, bert_model, device):\n",
        "    \"\"\"Предсказание на тестовых данных\"\"\"\n",
        "    model.eval()\n",
        "    test_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader, desc='Predicting on test'):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            bert_outputs = bert_model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            outputs = model(\n",
        "                batch['tfidf_features'],\n",
        "                bert_outputs.last_hidden_state[:, 0, :]\n",
        "            )\n",
        "            test_preds.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "    return test_preds\n",
        "\n",
        "# ==================== ОБУЧЕНИЕ ====================\n",
        "def train_epoch(model, train_loader, optimizer, scheduler, bert_model, device, epoch):\n",
        "    \"\"\"Одна эпоха обучения\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch + 1}')\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        bert_outputs = bert_model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask']\n",
        "        )\n",
        "        outputs = model(\n",
        "            batch['tfidf_features'],\n",
        "            bert_outputs.last_hidden_state[:, 0, :]\n",
        "        )\n",
        "        loss = F.binary_cross_entropy(outputs, batch['labels'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def train_and_validate():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Загрузка данных\n",
        "    train = pd.read_csv(\"/kaggle/input/train-dataset/train_v2_drcat_02.csv\")\n",
        "    test = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")\n",
        "\n",
        "    # Анализ данных (без утечек)\n",
        "    train = analyze_data(train, test)\n",
        "\n",
        "    # Подготовка тестового набора\n",
        "    test_texts = test['text'].tolist()\n",
        "\n",
        "    # Кросс-валидация\n",
        "    skf = StratifiedKFold(n_splits=CONFIG['training']['n_folds'], shuffle=True,\n",
        "                         random_state=CONFIG['training']['random_state'])\n",
        "\n",
        "    fold_metrics = []\n",
        "    test_preds = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['label'])):\n",
        "        print(f\"\\n=== Fold {fold + 1}/{CONFIG['training']['n_folds']} ===\")\n",
        "\n",
        "        # Разделение данных\n",
        "        train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "\n",
        "        # Инициализация TF-IDF и SVD для каждого фолда (избегаем утечки)\n",
        "        tfidf_vectorizer = TfidfVectorizer(**CONFIG['tfidf'])\n",
        "        tfidf_matrix = tfidf_vectorizer.fit_transform(train_df['text'])\n",
        "        svd = TruncatedSVD(**CONFIG['svd']).fit(tfidf_matrix)\n",
        "\n",
        "        # Даталоадеры\n",
        "        train_loader = DataLoader(\n",
        "            TextDataset(train_df['text'].tolist(), tfidf_vectorizer, svd,\n",
        "                       AutoTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased\"),\n",
        "                       labels=train_df['label'].tolist()),\n",
        "            batch_size=CONFIG['training']['batch_size'],\n",
        "            shuffle=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            TextDataset(val_df['text'].tolist(), tfidf_vectorizer, svd,\n",
        "                       AutoTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased\"),\n",
        "                       labels=val_df['label'].tolist()),\n",
        "            batch_size=CONFIG['training']['batch_size']\n",
        "        )\n",
        "\n",
        "        # Тестовый даталоадер (используем текущий tfidf/svd)\n",
        "        test_loader = DataLoader(\n",
        "            TextDataset(test_texts, tfidf_vectorizer, svd,\n",
        "                       AutoTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased\")),\n",
        "            batch_size=CONFIG['training']['batch_size']\n",
        "        )\n",
        "\n",
        "        # Инициализация моделей\n",
        "        bert_model = AutoModel.from_pretrained(\"/kaggle/input/bert-base-uncased\").to(device)\n",
        "\n",
        "        # Замораживаем все слои BERT, кроме последних N\n",
        "        for param in bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        for layer in bert_model.encoder.layer[-CONFIG['model']['bert_train_layers']:]:\n",
        "            for param in layer.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        model = HybridModel(\n",
        "            tfidf_dim=CONFIG['svd']['n_components'],\n",
        "            bert_dim=768,\n",
        "            hidden_dim=CONFIG['model']['hidden_dim'],\n",
        "            dropout=CONFIG['model']['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': model.parameters(), 'lr': CONFIG['model']['lr']['model']},\n",
        "            {'params': bert_model.parameters(), 'lr': CONFIG['model']['lr']['bert']}\n",
        "        ])\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=CONFIG['training']['warmup_steps'],\n",
        "            num_training_steps=len(train_loader) * CONFIG['training']['epochs']\n",
        "        )\n",
        "\n",
        "        # Ранняя остановка\n",
        "        best_auc = 0\n",
        "        patience_counter = 0\n",
        "\n",
        "        # Обучение\n",
        "        for epoch in range(CONFIG['training']['epochs']):\n",
        "            train_loss = train_epoch(model, train_loader, optimizer, scheduler, bert_model, device, epoch)\n",
        "\n",
        "            # Валидация\n",
        "            val_metrics = evaluate_model(model, val_loader, bert_model, device)\n",
        "            print(f\"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Val AUC: {val_metrics['auc']:.4f}\")\n",
        "            print(val_metrics['report'])\n",
        "\n",
        "            # Ранняя остановка\n",
        "            if val_metrics['auc'] > best_auc:\n",
        "                best_auc = val_metrics['auc']\n",
        "                patience_counter = 0\n",
        "                torch.save(model.state_dict(), f'best_model_fold{fold}.pt')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "                if patience_counter >= CONFIG['training']['patience']:\n",
        "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "                    break\n",
        "\n",
        "        # Загрузка лучшей модели для фолда\n",
        "        model.load_state_dict(torch.load(f'best_model_fold{fold}.pt'))\n",
        "\n",
        "        # Предсказание на тесте\n",
        "        fold_test_preds = predict_test(model, test_loader, bert_model, device)\n",
        "        test_preds += np.array(fold_test_preds) / CONFIG['training']['n_folds']\n",
        "\n",
        "        # Сохранение метрик фолда\n",
        "        fold_metrics.append(best_auc)\n",
        "\n",
        "    # Финализация результатов\n",
        "    print(\"\\n=== ИТОГОВЫЕ МЕТРИКИ ===\")\n",
        "    print(f\"Средний AUC по фолдам: {np.mean(fold_metrics):.4f} (±{np.std(fold_metrics):.4f})\")\n",
        "    print(f\"Лучший AUC: {np.max(fold_metrics):.4f}\")\n",
        "\n",
        "    # Сохранение предсказаний\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'generated': test_preds\n",
        "    })\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "    print(\"\\nПредсказания сохранены в submission.csv\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_validate()"
      ]
    }
  ]
}